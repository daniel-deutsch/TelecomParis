{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_LeNet5_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafPdtuncbq7"
      },
      "source": [
        "<h2><center>MNIST classification using <i>LeNet5</i></center></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuysP38WTRmq"
      },
      "source": [
        "# As a first step, we may want to switch to a GPU-acceperated VM\n",
        "# In the menu: Runtime->Change runtime type->Hardware Accelerator->GPU.\n",
        "#\n",
        "# This will thest if we have a GPU-equipped VM and return some useful system-level information\n",
        "#!nvidia-smi\n",
        "\n",
        "# Which GNU/Linux distribution is installed on our VM ?\n",
        "#!lsb_release -a\n",
        "\n",
        "# Which version of the Linux kernel our VM has ?\n",
        "#!uname -a\n",
        "\n",
        "# How much free memory our VM has ?\n",
        "#!free -h\n",
        "\n",
        "# Which storage facilities our VM has ?\n",
        "#!mount\n",
        "\n",
        "# Which python version our VM has installed ?\n",
        "#!python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4VrCB5La5rD"
      },
      "source": [
        "# Importing Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlKZ3Hnas7B4"
      },
      "source": [
        "# Importing the Keras main module and the tensorflow backend\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(\"Using tensorflow version \" + str(tf.__version__))\n",
        "print(\"Using keras version \" + str(keras.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_QLz9_jbRZq"
      },
      "source": [
        "# Loading and preparing the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG83hGyVmijn"
      },
      "source": [
        "#@title\n",
        "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
        "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "print(\"Shape of the train images is \", train_images.shape)\n",
        "print(\"Shape of the train labels is \", train_labels.shape)\n",
        "print(\"Shape of the test images is \", test_images.shape)\n",
        "print(\"Shape of the test labels is \", test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VAu7oW0Zu4"
      },
      "source": [
        "# Let us visualize the first training sample using the Gnuplot library\n",
        "from matplotlib import pyplot as plt\n",
        "imageIndex = 0\n",
        "print(\"Label for \" + str(imageIndex) + \"-th train image is: \" + str(train_labels[0]))\n",
        "plt.imshow(train_images[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQbkllF8mnaf"
      },
      "source": [
        "# The ground truth labels need to be converted to the one-hot encoding format via to_categorical\n",
        "from keras.utils.np_utils import to_categorical\n",
        "imageIndex = 0\n",
        "#print(\"This is the native \" + str(imageIndex) + \"-th train label: \" + str(train_labels[0]))\n",
        "train_labels = to_categorical(train_labels)\n",
        "#print(\"This is the one-hot encoding of the \" + str(imageIndex) + \"-th train label: \" + str(train_labels[0]))\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptTRSDo5nJyZ"
      },
      "source": [
        "# We need to reshape to proper images with 1 color channel according to the tensorflow backend NWHC scheme\n",
        "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
        "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols,1 )\n",
        "print('train_images shape:', train_images.shape)\n",
        "print('test_images shape:', test_images.shape)\n",
        "\n",
        "# Casting pixels from uint8 to float32 to allow normalization\n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "# First we compute the mean pixel intensity and variance over the training set\n",
        "train_mean = train_images.mean()\n",
        "train_std = train_images.std()\n",
        "\n",
        "# then we normalize the images over the train set staistics\n",
        "train_images = (train_images - train_mean)/train_std\n",
        "test_images = (test_images - train_mean)/train_std\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwm1OFOtc4uU"
      },
      "source": [
        "# Defining the neural network architecture (i.e., the network model)\n",
        "Create a LeNet5-like convolutional neural network taking in input the images as matrices of pixels and suitable to classify each image across 10 different classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnd3q1V3nk8v"
      },
      "source": [
        "# The Sequential module is sort of a container for more complex NN elements and\n",
        "# defines a loop-less NN architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "output_shape = 10\n",
        "\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "# convolution kernel size\n",
        "kernel_size = (5, 5)\n",
        "# Number of filters in first convolutional layer\n",
        "num_kernel_first_conv_layer = 6\n",
        "# Number of filters in second convolutional layer\n",
        "num_kernel_second_conv_layer = 16\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#First Convolve-and-pool block\n",
        "model.add(Convolution2D(num_kernel_first_conv_layer, (kernel_size[0], kernel_size[1]), input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "#Second Convolve-and-pool block\n",
        "model.add(Convolution2D(num_kernel_second_conv_layer, (kernel_size[0], kernel_size[1])))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "# Turns the sequence of featuremaps into a linear array of features\n",
        "model.add(Flatten())\n",
        "\n",
        "# First fully connected hidden layer with 120 neurons\n",
        "model.add(Dense(120))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Second fully connected hidden layer with 84 neurons\n",
        "model.add(Dense(84))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(output_shape))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_F7zfhY9NJw"
      },
      "source": [
        "Instantiate a SGD optimizer with a tentative LR of 10^-4 and using the appropriate loss function and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoecwIXr9PZf"
      },
      "source": [
        "# The optimizers module provides a number of optimization algorithms for updating\n",
        "# a netwok parameters accoridng to the computed error gradints\n",
        "from keras import optimizers\n",
        "\n",
        "# Defining our optimizer as the standard stochastic gradient optimizer\n",
        "optimizer=optimizers.SGD(lr=1e-4)\n",
        "\n",
        "# This initializes the SGD optimizer above for the LeNet5 architecture\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Let us have a look at the model topology after compiling the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AWUAW4idF3D"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTHrbb7uFYWz"
      },
      "source": [
        "# This is where the actual training-testing happens\n",
        "# Dimension of the batch size (number of images over which error gradients are averaged)\n",
        "batch_size = 64\n",
        "# Number of epochs we want to train\n",
        "epochs = 10\n",
        "\n",
        "# This structure holds the training history for later plotting\n",
        "history = {}\n",
        "history['loss'] = []\n",
        "history['val_loss'] = []\n",
        "history['acc'] = []\n",
        "history['val_acc'] = []\n",
        "\n",
        "\n",
        "# Creating a batch preprocessor for feeding the train and test data in batches\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "myDatagen = ImageDataGenerator()\n",
        "# Compute quantities required for feature-wise normalization\n",
        "myDatagen.fit(train_images)\n",
        "\n",
        "# Cycling through the epochs\n",
        "for e in range(epochs):\n",
        "    lossEpochTrain = 0\n",
        "    lossEpochTest = 0\n",
        "    accuracyEpochTrain = 0\n",
        "    accuracyEpochTest = 0\n",
        "\n",
        "    # Training over the training samlpes, batch by batch\n",
        "    batchCntTrain = 0\n",
        "    for images_batch, labels_batch in myDatagen.flow(train_images, train_labels, batch_size=batch_size):\n",
        "        batch_history = model.train_on_batch(images_batch, labels_batch)\n",
        "        lossEpochTrain += batch_history[0]\n",
        "        accuracyEpochTrain += batch_history[1]\n",
        "        batchCntTrain += 1\n",
        "        # break the loop or generator loops indefinitely\n",
        "        if batchCntTrain >= len(train_images) / batch_size:\n",
        "            break\n",
        "\n",
        "    # Testing over the training samlpes, batch by batch\n",
        "    batchCntTest = 0\n",
        "    for images_batch, labels_batch in myDatagen.flow(test_images, test_labels, batch_size=batch_size):\n",
        "        batch_history = model.test_on_batch(images_batch, labels_batch)\n",
        "        lossEpochTest += batch_history[0]     \n",
        "        accuracyEpochTest += batch_history[1]\n",
        "        batchCntTest += 1\n",
        "        if batchCntTest >= len(test_images) / batch_size:\n",
        "            break\n",
        "\n",
        "    print ('Epoch %d / %d lossTrain %.3f lossTest %.3f accuracyTrain %.3f accuracyTest %.3f' %(int(e), epochs, lossEpochTrain/batchCntTrain, lossEpochTest/batchCntTest, accuracyEpochTrain/batchCntTrain, accuracyEpochTest/batchCntTest))\n",
        "    history['loss'].append(lossEpochTrain/batchCntTrain)\n",
        "    history['val_loss'].append(lossEpochTest/batchCntTest)\n",
        "    history['acc'].append(accuracyEpochTrain/batchCntTrain)\n",
        "    history['val_acc'].append(accuracyEpochTest/batchCntTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODUc5Bq_dMEq"
      },
      "source": [
        "# Visualizing the network performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdJrRbyariEw"
      },
      "source": [
        "# We now want to plot the train and validation loss functions and accuracy curves\n",
        "from matplotlib import pyplot as plt\n",
        "#print(history.history.keys())\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history['acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr4TdWoEoDzi"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "Note down the performance of the trained network in terms of training and validation accuracy as a reference. Then, experiment as follow and compare performance with the reference scenario.\n",
        "\n",
        "*   **Filter size**: experiment with square filters of different size and compare performance with reference scenario.\n",
        "*   **Number of filters**: experiment increasing the number of filters in the first and second layer and find the maximum number of filters the network can tolerate before overfitting to the training samples.\n",
        "* **Padding**: experiment withnarrow and wide convolutions: what changes in terms of featuremap size ?\n",
        "*  **Pooling layers**: expeirment with different pooling layers (maxpooling and avgpooling): which one yield the best performance ?\n",
        "What happens if the pooling layers are removed altogether in terms of comlexity-performance tradeoff ?\n",
        "* **Pooling-less architectures**: Modify the network architecture to obtain a twofold reduction of each featuremap without resorting to pooling layers (hint: take insipiration from the ResNet architecture).\n",
        "* **Confusion analysis**: Using the proper metric  from sklearn, check which character is most frequently confused with which: can you explain why ?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0C1J6R1Elfc"
      },
      "source": [
        "# We now want to plot the confusion matrix using sklearn.metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(test_images)\n",
        "# Mind that confusion_matrix requires\n",
        "matrix = confusion_matrix(test_labels.argmax(axis=1), predictions.argmax(axis=1))\n",
        "print (matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IOI6Tl1dS13"
      },
      "source": [
        "#Saving the training results\n",
        "\n",
        "Save the best trained model (topology, parameters), and all the related side information required to deploy the trained model later on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M68i9PeDdZrG"
      },
      "source": [
        "# Create a directory for saving both the trained model and side information\n",
        "import os\n",
        "save_dir = os.path.join(os.getcwd(), 'trained_lenet5_mnist')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Save model and weights\n",
        "model_name = 'model.h5'\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Saving mean and standard deviation information as a CSV file\n",
        "import csv\n",
        "model_name = 'std_dev.csv'\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "w = csv.writer(open(model_path, \"w\"))\n",
        "dict={}\n",
        "dict['mean'] = train_mean\n",
        "dict['std'] = train_std\n",
        "for key, val in dict.items():\n",
        "    w.writerow([key, val])\n",
        "print('Saved side information at %s ' % model_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}