{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBFqSEkKqpCN"
   },
   "source": [
    "# Lab Deep Learning/ Recurrent Neural Networks/ in keras\n",
    "\n",
    "## Using Many-to-One for movie rating predicton\n",
    "\n",
    "**Author: geoffroy.peeters@telecom-paris.fr**\n",
    "\n",
    "**Version**: 2020/10/05 (changed to tensorfow.keras)\n",
    "    \n",
    "For any remark or suggestion, please feel free to contact me.\n",
    "\n",
    "## Objective:\n",
    "We will implement two different networks to perform automatic rating (0 or 1) of a movie given the text of its review.\n",
    "We will use the ```imdb``` (internet movie database) dataset.\n",
    "\n",
    "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmkCSNaXLqjh"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "\n",
    "colab = False\n",
    "student = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5Yp4OQVvUtr"
   },
   "source": [
    "## Parameters of the model\n",
    "\n",
    "-  We only consider the ```top_words``` first words in the word dictionary\n",
    "- We truncate/zero-pad each sequence a length ```max_review_length```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4C_Pv7rYvRkM"
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "max_review_length = 100\n",
    "INDEX_FROM = 2\n",
    "embedding_vector_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsNcRimyLzgP"
   },
   "source": [
    "## Import IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import the IMDB data and only consider the ``top_words``` most used words\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)\n",
    "np.load.__defaults__=(None, False, True, 'ASCII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSc5LmksOLyr"
   },
   "source": [
    "## Data content\n",
    "\n",
    "- ```X_train``` and ```X_test``` are numpy arrays of lists. \n",
    "  - each item in a list is the index in the word dictionary. So that a list is the sequence of index of words.\n",
    "\n",
    "- ```y_train``` and ```y_test``` are a numpy arrays of the same dimension as ```X_train``` and ```X_test``` \n",
    "  - they contains the values 0 (bad movie) or 1 (good movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "WouODCPrtiuu",
    "outputId": "064e4f93-d072-4b95-abf7-a5df05389ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_train): <class 'numpy.ndarray'>\n",
      "number of training sequences: X_train.shape: (25000,)\n",
      "type(X_train[0]): <class 'list'>\n",
      "length of the first training sequence: len(X_train[0]): 218\n",
      "length of the second training sequence: len(X_train[1]): 189\n",
      "list of data of the first training sequence: X_train[0]: [1, 13, 21, 15, 42, 529, 972, 1621, 1384, 64, 457, 4467, 65, 3940, 3, 172, 35, 255, 4, 24, 99, 42, 837, 111, 49, 669, 2, 8, 34, 479, 283, 4, 149, 3, 171, 111, 166, 2, 335, 384, 38, 3, 171, 4535, 1110, 16, 545, 37, 12, 446, 3, 191, 49, 15, 5, 146, 2024, 18, 13, 21, 3, 1919, 4612, 468, 3, 21, 70, 86, 11, 15, 42, 529, 37, 75, 14, 12, 1246, 3, 21, 16, 514, 16, 11, 15, 625, 17, 2, 4, 61, 385, 11, 7, 315, 7, 105, 4, 3, 2222, 2, 15, 479, 65, 3784, 32, 3, 129, 11, 15, 37, 618, 4, 24, 123, 50, 35, 134, 47, 24, 1414, 32, 5, 21, 11, 214, 27, 76, 51, 4, 13, 406, 15, 81, 2, 7, 3, 106, 116, 2, 14, 255, 3, 2, 6, 3765, 4, 722, 35, 70, 42, 529, 475, 25, 399, 316, 45, 6, 3, 2, 1028, 12, 103, 87, 3, 380, 14, 296, 97, 31, 2070, 55, 25, 140, 5, 193, 2, 17, 3, 225, 21, 20, 133, 475, 25, 479, 4, 143, 29, 2, 17, 50, 35, 27, 223, 91, 24, 103, 3, 225, 64, 15, 37, 1333, 87, 11, 15, 282, 4, 15, 4471, 112, 102, 31, 14, 15, 2, 18, 177, 31]\n",
      "maximum length of a training sequence: 2494\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"type(X_train):\", type(X_train))\n",
    "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
    "print(\"type(X_train[0]):\",type(X_train[0]))\n",
    "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
    "print(\"length of the second training sequence: len(X_train[1]):\",len(X_train[1]))\n",
    "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
    "len_list = [len(train) for train in X_train]\n",
    "print(\"maximum length of a training sequence:\", max(len_list))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(len_list, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2I-cEKUh_HM4"
   },
   "source": [
    "## Details of how the reviews are encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "XcOwiMUT_HM5",
    "outputId": "77456808-95e8-4886-cfbf-c6109926abe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in X_train[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Hfl42LGCugWB",
    "outputId": "0f9dbbce-59ec-4bf8-a57c-337d2e048a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(y_train): <class 'numpy.ndarray'>\n",
      "y_train.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"type(y_train):\", type(y_train))\n",
    "print(\"y_train.shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "iVw65PNNuobX",
    "outputId": "04fa51ea-99ca-4f64-a185-4a4ec80bca0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape: (25000,)\n",
      "y_test.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V18OA7oQNH3c"
   },
   "source": [
    "## Data processing\n",
    "\n",
    "Sequences (represented as a list of values) in ```X_train``` represent the reviews.\n",
    "They can have different length.\n",
    "To train the network we should modify them so that they all have the same length.\n",
    "We do this by:\n",
    "- truncating the ones that are too long\n",
    "- padding-with-zero them the ones that are too short.\n",
    "\n",
    "This is obtained using ```sequence.pad_sequences``` of keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "JhmiHsOGoRwT",
    "outputId": "88836720-32a2-43cb-e4f2-5e9256c16c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train[0]): 100\n",
      "len(X_train[1]): 100\n",
      "X_train[0]: [1414   32    5   21   11  214   27   76   51    4   13  406   15   81\n",
      "    2    7    3  106  116    2   14  255    3    2    6 3765    4  722\n",
      "   35   70   42  529  475   25  399  316   45    6    3    2 1028   12\n",
      "  103   87    3  380   14  296   97   31 2070   55   25  140    5  193\n",
      "    2   17    3  225   21   20  133  475   25  479    4  143   29    2\n",
      "   17   50   35   27  223   91   24  103    3  225   64   15   37 1333\n",
      "   87   11   15  282    4   15 4471  112  102   31   14   15    2   18\n",
      "  177   31]\n"
     ]
    }
   ],
   "source": [
    "# --- truncate and pad input sequences\n",
    "\n",
    "if student:\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, value=0)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, value=0)\n",
    "\n",
    "print(\"len(X_train[0]):\", len(X_train[0]))\n",
    "print(\"len(X_train[1]):\", len(X_train[1]))\n",
    "print(\"X_train[0]:\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlrDTuk5K65Q"
   },
   "source": [
    "# First model\n",
    "\n",
    "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_RNN_01.png\">\n",
    "\n",
    "In the first model, we will simply \n",
    "- learn a word embedding  (```Embedding``` layer in keras) and apply it to each item of the sequence, \n",
    "  -  in keras, embedding is not a matrix going from one-hot-encoding to embedding, but is a layer that goes from index-in-word-dictionary to embedding\n",
    "  - the embedding goes from ```top_words``` dimensions to  ```embedding_vector_length``` dimensions\n",
    "- average the embedding obtained for each word of a sequence over all words of the sequence (you should use ```K.mean``` and ```Lambda``` from the keras backend)\n",
    "- apply a fully connected (```Dense``` layer in keras) which output activation is a sigmoid (predicting the 0 or 1 rating)\n",
    "\n",
    "We will code this model \n",
    "- First, using the Sequential API of keras (https://keras.io/models/sequential/)\n",
    "- Secondly, using the Functional API of keras (https://keras.io/getting-started/functional-api-guide/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "ufW00TGcs3Jj",
    "outputId": "4482bfbf-6b61-4a56-969b-6820d6506e44"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "zspaUptgtW9l",
    "outputId": "3ddf41b9-d53b-4ffe-cd3a-18c9676c86e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# --- create the model\n",
    "# CODE-RNN1-2\n",
    "if student:\n",
    "    # --- START CODE HERE (02)\n",
    "    # --- Using the Sequential API\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(top_words, embedding_vector_length, input_length=max_review_length),\n",
    "            Lambda(K.mean, arguments={'axis': 1}),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "pFXz4AS6tawQ",
    "outputId": "df5a3960-fad6-4ac2-dd85-36f021e75ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2s 72us/sample - loss: 0.6380 - acc: 0.7226 - val_loss: 0.5616 - val_acc: 0.7767\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 2s 71us/sample - loss: 0.4862 - acc: 0.8146 - val_loss: 0.4403 - val_acc: 0.8194\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 2s 72us/sample - loss: 0.3940 - acc: 0.8440 - val_loss: 0.3848 - val_acc: 0.8368\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 2s 71us/sample - loss: 0.3471 - acc: 0.8583 - val_loss: 0.3592 - val_acc: 0.8441\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 2s 68us/sample - loss: 0.3187 - acc: 0.8706 - val_loss: 0.3453 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 2s 71us/sample - loss: 0.2990 - acc: 0.8780 - val_loss: 0.3378 - val_acc: 0.8514\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 2s 68us/sample - loss: 0.2844 - acc: 0.8840 - val_loss: 0.3344 - val_acc: 0.8538\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 2s 68us/sample - loss: 0.2726 - acc: 0.8891 - val_loss: 0.3335 - val_acc: 0.8555\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 2s 69us/sample - loss: 0.2631 - acc: 0.8938 - val_loss: 0.3353 - val_acc: 0.8530\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 2s 71us/sample - loss: 0.2551 - acc: 0.8965 - val_loss: 0.3374 - val_acc: 0.8535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa58974ad10>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBqyzLJRUIsC"
   },
   "source": [
    "## Results\n",
    "\n",
    "After only 3 epochs, you should obtain an accuracy around 84% for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nCALyP-Q_HNH",
    "outputId": "76ee8222-9ec7-442f-9a52-1f1aaf055780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.35%\n"
     ]
    }
   ],
   "source": [
    "# --- Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRP-h4Xr_HNJ"
   },
   "source": [
    "## Using the trained embedding to find equivalence between words\n",
    "\n",
    "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
    "Use the weights of the ```Embedding``` layer to find the most similar words to ```great```. We will use an Euclidean distance for that.\n",
    "- Retrieve the weights of the ```Embedding layer```\n",
    "- Get the position of ```great``` in the dictionary\n",
    "- Get the word-embedding of ```great```\n",
    "- Find (using Euclidean distance), the closest embedded-words to ```great```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "7xMubRqJ_HNJ",
    "outputId": "22ca653e-30ba-4b74-8fea-359f5a4db4d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantastic\n",
      "friendship\n",
      "rare\n",
      "favorites\n",
      "delight\n",
      "outstanding\n"
     ]
    }
   ],
   "source": [
    "if student:\n",
    "    E = model.layers[0].get_weights()[0]\n",
    "    great_id = word_to_id['great']\n",
    "    great_embedding = E[great_id, :]\n",
    "    \n",
    "    distances = {\n",
    "        idx: np.linalg.norm(great_embedding - embedding) for idx, embedding in enumerate(E) if idx != great_id\n",
    "    }\n",
    "    \n",
    "    distances_sorted = {k:v for k, v in sorted(distances.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    for idx, key in enumerate(distances_sorted.keys()):\n",
    "        print(id_to_word[key])\n",
    "        \n",
    "        if idx==5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK9e5Eo1Ks2a"
   },
   "source": [
    "# Second model\n",
    "\n",
    "In the second model, we will replace\n",
    "- the average over the sequence of the obtained embedding\n",
    "- by a RNN layer (more precisely an ```LSTM```) in a Many-To-One configuration with $n_a=128$\n",
    "\n",
    "We will code this model \n",
    "- First, using the Sequential API of keras (https://keras.io/models/sequential/)\n",
    "- Secondly, using the Functional API of keras (https://keras.io/getting-started/functional-api-guide/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwoXuOqqVDOy"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "7dl-CSMKoViX",
    "outputId": "1cf3e91e-0f7f-4c3f-e982-b384b377c3dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 242,561\n",
      "Trainable params: 242,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# --- create the model\n",
    "\n",
    "if student:\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Embedding(top_words, embedding_vector_length, input_length=max_review_length),\n",
    "            LSTM(128, activation='sigmoid'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "-bp7PzX7oXtB",
    "outputId": "bc88fcb0-e2c6-47d6-def8-c054d536e072",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.5790 - acc: 0.6601 - val_loss: 0.3749 - val_acc: 0.8358\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 105s 4ms/sample - loss: 0.3410 - acc: 0.8524 - val_loss: 0.3404 - val_acc: 0.8511\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 110s 4ms/sample - loss: 0.2867 - acc: 0.8824 - val_loss: 0.3622 - val_acc: 0.8478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa52d9ebb10>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- compile and fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1LN_fjMWBHJ"
   },
   "source": [
    "## Results\n",
    "\n",
    "After only 3 epochs, you should obtain an accuracy around 88% for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RlMEKRbzoavm",
    "outputId": "b1b21ab7-b167-4c0b-9023-8cb105b7ed8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.03%\n"
     ]
    }
   ],
   "source": [
    "# --- Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVK5sGgF_HNX"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for \n",
    "- 1) Data Pre-Processing (01)\n",
    "- 2) First model using the Sequential API (02)\n",
    "- 3) First model using the Functional API (03)\n",
    "- 4) Find equivalence between words (04)\n",
    "- 5) Second model using the Sequential API (05)\n",
    "- 6) Second model using the Functional API (06)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL1_20192020_Lab_keras_imdb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
